### ハンズオンラボ: Databricksでエージェントシステムを構築しよう

このラボは2つのパートに分かれています。**パート1**では、カスタマーサービスのシナリオ向けに様々なツールコールを持つDatabricksエージェントを構築・テストします。**パート2**では、製品に関する質問に答えるよりシンプルなエージェントを作成し、その性能評価に注力します。

---

### パート1: 最初のエージェントを設計しよう
##### ノートブック: 01_create_tools\01_create_tools

#### 1.1 ツールの構築
- **SQL関数**  
  - カスタマーサービスの返品ワークフローを処理するために重要なデータへアクセスするクエリを作成します。  
  - これらのSQL関数はノートブックやエージェントから簡単に呼び出せます。
- **シンプルなPython関数**  
  - 言語モデルの一般的な制限に対処するPython関数を作成します。  
  - これを「ツール」として登録し、エージェントが必要に応じて呼び出せるようにします。

#### 1.2 LLMとの統合 [AI Playground]
- **ツールとLLMの組み合わせ**  
  - Databricks AI PlaygroundでSQL/Pythonツールと大規模言語モデル（LLM）を組み合わせます。  
  - モデル: Claude 3.7 Sonnet

#### 1.3 エージェントのテスト [AI Playground]
- **システムプロンプト**: すべての会社方針が満たされるまでツールを呼び出してください。
- **質問例**: 当社の方針に基づき、キュー内の最新の返品を受け付けるべきですか？
  - エージェントの段階的な推論と最終回答を観察しましょう。
- **MLflowトレースの確認**  
  - MLflowでエージェントの実行を確認し、各ツールの呼び出し方法を理解します。  
  
---

### パート2: エージェント評価
##### ノートブック: 02_agent_eval\agent

#### 2.1 新しいエージェントとリトリーバーツールの定義
- **ベクター検索**  
  - 関連する製品ドキュメントを取得するためのベクター検索インデックスを事前に用意しています。  
  - このVSインデックスは agents_lab.product.product_docs_index にあります。
- **リトリーバー関数の作成**  
  - このベクター検索インデックスを関数でラップし、LLMが製品情報を検索できるようにします。  
  - 最終回答にも同じLLMを使用します。

##### ノートブック: 02_agent_eval/driver

#### 2.2 評価用データセットの定義
- **提供データセットの利用**  
  - サンプル評価データセットを活用し、エージェントが製品の質問に答える能力をテストします。  
  - （オプション）[合成データ生成](https://www.databricks.com/blog/streamline-ai-agent-evaluation-with-new-synthetic-data-capabilities)も試してみましょう。

#### 2.3 エージェントの評価
- **`MLflow.evaluate()`の実行** 
  - MLflowがエージェントの回答と正解データセットを比較します。  
  - LLMベースの判定者が各回答をスコア付けし、フィードバックを収集します。

#### 2.4 改善と再評価
- **リトリーバーの改善**  
  - 評価フィードバックに基づき、リトリーバー設定（k=5→k=1）を調整します。  
- **再評価の実施**  
  - 新しいMLflowランを開始
  - 再度`MLflow.evaluate()`を実行し、結果を比較します。  
  - MLflow評価UIでパフォーマンス向上を確認しましょう

---

### 次のステップ
- **さらなるツールの活用**: API、高度なPython関数、追加のSQLエンドポイントでエージェントを拡張しましょう。  
- **本番展開**: 継続的インテグレーション/デリバリー（CI/CD）を導入し、MLflowでパフォーマンスを監視、モデルバージョンを管理しましょう。

---

Databricksでエージェントシステムを構築・評価できたこと、おめでとうございます！